
-> There are various ways of increasing the speed of a processor:
1. Increase the size of the data bus (costly).
2. Increase clock speed (physically impossible???).
3. Work on multiple instructions at same time (PIPELINING!!!!)

-> Pipelining is the main reason why processors have grown to work on these tremendous speeds.

#### What even is a pipeline?
A pipeline is when you're working on multiple instructions, how is this done?

Consider a 5 instruction program to be executed by both 8085 (Non-pipelined Processor)  and 8086 (2-stage Pipelined Processor):


![[8085 vs 8086 Pipeline.png]]

The process begins normally with the fetching of 1st instruction i.e, $F_1$.

When instruction 1 is being executed, i.e, $E_1$ is being executed INSIDE the $\mu P$, the external buses are free. Therefore instead of waiting for completion of $E_1$ to fetch instruction 2. We do it simultaneously, i.e, $E_1$ and $F_2$ happen at the same time.

But how is this possible? What if $E_1$ is an instruction which REQUIRES the use of the [[1. Basics of 8086#System Bus|System Buses]]?

Well actually. The Fetch and Execute of an instruction are done by 2 separate independent units in the $\mu P$ called the Bus Interface Unit, and the Execution Unit.

More on those later.

A more detailed/accurate yet confusing (confusion will be solved later) view of 8086's pipelining is given in the [Intel 8086 Instruction Manual](https://edge.edx.org/c4x/BITSPilani/EEE231/asset/8086_family_Users_Manual_1_.pdf#page=19).

Intel in it's manual didn't mention anything about the "Decode" of the instruction, and we don't even start considering the MEMR MEMW IOR IOW operations that deeply, so just think of the decode as something that happens between the Fetch and Execute in a single clock pulse, and we don't care for this small amount of time.

Personally: think of decode as a part of the fetch.


## Pipelining in more modern Architectures:

What Intel's engineers realize is: if we can break the instruction cycle \[Fetch-Decode-Execute]  into multiple parts. 
Eg: Memory Reads, Address Generations, Decodes, Executes, Result Stores, etc.

And make INDEPENDENT Units for each of these sub-tasks in an instruction cycle, we can work on multiple instructions parallelly without any problems. The "deeper" the pipeline, the more instructions you can work on at once.

For a 2-stage pipeline you can work on 2 instructions at once
For a 3-stage pipeline you can work on 3 instructions at once
For a n-stage pipeline you can work on n instructions at once.

## Super Scalar Pipelining:

Intel's 80486 introduced a 5 Stage Pipeline, which was improved by Pentium.

In Pentium, there is a 5-stage Pipeline, and physically on the chip, 2 such pipes exist.
So actually, Pentium worked on 10 different instructions at the same time.

This concept of having Multiple Pipes is called Super Scalar Pipelining.

How would this work though?

You have 2 Pipes, say P1 and P2, each pipe has 5-stages, let the stages be named A, B, C, D, E, where A is the first stage of the instruction cycle and E is the last.
	Suppose you have 10 instructions.
If instruction 1 is at stage A in P1, now it moves to stage B in P1, 
What should happen to Instruction 2? Is it loaded in P1 stage A, or in P2 stage A??.

Think about it, if we wait for the 1st Pipe to fill completely in order, and then fill the 2nd pipe in order i.e, 

P1 = $A_5$ $B_4$ $C_3$ $D_2$ $E_1$ 
P2 = $A_{10}$ $B_{9}$ $C_{8}$ $D_{7}$ $E_{6}$ 

This would mean that after Instruction 1, Directly Instruction 6 will be executed, which makes no sense, We would like for Instruction 1 and 2 to execute simultaneously (almost).

Imagine that each pipe is executing independently and MOST IMPORTANTLY PARALLELLY.
So that means we have the capability to do $E_1$ and $E_2$ together.

To achieve this, we insert each instruction alternately in each pipeline.

So this would look like:

Instruction 1 in P1 and Instruction 2 in P2
Instruction 3 in P1 and Instruction 4 in P2
Instruction 5 in P1 and Instruction 6 in P2
and so on...

P1 = -> $A_9$ $B_7$ $C_5$ $D_3$ $E_1$  ->
P2 = -> $A_{10}$ $B_{8}$ $C_{6}$ $D_{4}$ $E_{2}$ ->

now $E_1$ and $E_2$ are happening together.

Near 2000's Intel came up with the "NetBurst" architecture.
In which there are 4 Pipes, and each Pipe has a 20-stage Pipeline.
Which means it could work on 80 Instructions at once.


### Drawbacks of Pipelining:

1. **Data Dependency**: In multiple-piped architectures, where multiple instructions are executed at the same time Eg: $E_1$ and $E_2$ happen exactly at same time. But also, the $E_2$ is dependent on the result of $E_1$. It would/SHOULD have to wait for $E_1$ to finish.
		This is solved by the Assembler Programs/smart Programming by inserting some NOPs (NO OPERATION) instructions, not a huge cost/waste to solve this problem.
		
	The data dependency issue is not a major problem, but,
	
2. **BRANCHING**: is the major drawback of Pipelining.
		Eg: You simultaneously worked on 80 instructions starting at address `0x4000` so instructions at addresses `0x4000` to `0x4050`. 
		Now when $E_1$ happens, your pipes are filled with all instructions from 1 to 80, i.e, $A_{80}$.
		What if instruction 1 said "`JMP 0x8000`". Now program is at address `0x8000`, what about the 79 other instructions loaded in the pipes? we don't need them anymore we need to start fresh with the instructions starting from address `0x8000`. So you have to "flush" out the pipeline, and all the work done on those other 79 instructions which were partly processed will go to waste.

To solve this Branching issue, Intel introduced the "Branch Prediction Algorithm" since Pentium and has been implemented in all further processors.


#### Branch Prediction Algorithm:
In very simplified words.

It is is a hardware technique in which the processor maintains a history called "Branch Target Buffer", which stores a number (256 for Pentium) entries, each entry shows the instruction of which address caused a jump to which address, so that when this address's instruction in brought into the pipeline, instead of the next instruction, we start bringing the target addresses' instructions in the pipe,

Eg: there is a loop of 100 iterations

```

	MOV CX, 100 ;put 100 count in CX
loop:
	INST1
	INST2
	...
	...
	...
	DCX CX ;decrement count
	JNE loop ;if count != 0, go back
	
	OTHER INST1
	OTHER INST2
	...
	...
```

The first time the `JNE loop` instruction is encountered.

All the other `OTHER INST`'s  have to be flushed out.
=> 1 time flush.

Now the $\mu P$ will predict that when we reach the `JNE loop` is reached, load the pipe with instructions staring from the address of the label `loop:`
$x$ number of correct predictions.

When `CX` actually becomes 0.

$\mu P$ predicts to go to `loop:`, and fills pipe from there, but it won't have to.
So the instructions from `loop:` have to be flushed out from the pipe and the `OTHER INSTS` have to be loaded in.
=> 1 time flush.

There were 100 expected flushes because of the jumps/branching in total, out of which we made only 2 because of branch prediction.

Flushing out only 2 times no matter if it was an 80 instruction pipeline => 160 partly processed instructions, is still a HUGE cost save over the 100 expected flushes which would've wasted around 8000 or 6400 so partly processed instructions.

Branch Prediction Algorithm gives around 98% to 99% accuracy.